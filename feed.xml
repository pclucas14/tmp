<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://www.cs.mcgill.ca/~lpagec/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.cs.mcgill.ca/~lpagec/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-03-24T09:08:05-04:00</updated><id>https://www.cs.mcgill.ca/~lpagec/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">EWC derivation</title><link href="https://www.cs.mcgill.ca/~lpagec/blog/2020/math/" rel="alternate" type="text/html" title="EWC derivation" /><published>2020-07-01T11:12:00-04:00</published><updated>2020-07-01T11:12:00-04:00</updated><id>https://www.cs.mcgill.ca/~lpagec/blog/2020/math</id><content type="html" xml:base="https://www.cs.mcgill.ca/~lpagec/blog/2020/math/"><![CDATA[<p>In the original \(\textbf{Elastic Weight Consolidation}\) <a href="https://arxiv.org/pdf/1612.00796.pdf">paper</a>, the authors show a neat result saying that to fit a model sequentially on two datasets,  \(\mathcal{D}_1\) and \(\mathcal{D}_2\), you don’t need to store data from  \(\mathcal{D}_1\), provided you know the posterior of your parameters after seeing \(\mathcal{D}_1\), i.e. \(p(\theta \mid \mathcal{D}_1)\). In math,</p>

\[\log p(\theta \mid \mathcal{D} ) = \log p(\theta \mid \mathcal{D}_1) + \log p(\mathcal{D}_2 \mid \theta) - \log p(\mathcal{D}_2)\]

<p>where,  \(\mathcal{D}=\mathcal{D}_1 \cup \mathcal{D}_2\). This is super cool, because \(\log p(\theta \mid \mathcal{D} )\) is exactly what you wish to compute (a model trained on the whole dataset), and the R.H.S does not require data from \(\mathcal{D}_1\). When I first read the paper This equation seemed somewhat arbitrary, so here is the full derivation.</p>

<p>Starting with Baye’s Rule, we have</p>

\[\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D} \mid \theta) + \log p(\theta) - \log p(\mathcal{D})\]

<p>Let’s expand out  \(\mathcal{D}=\mathcal{D}_1 \cup \mathcal{D}_2\).</p>

\[\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D}_1 \cup \mathcal{D}_2 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1 \cup \mathcal{D}_2)\]

<p>Note that samples making up the dataset are independent, so we can split the first and last terms</p>

\[\log p(\theta \mid \mathcal{D}) = \log p(\mathcal{D}_1 \mid \theta) + \log p(\mathcal{D}_2 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1) - \log p( \mathcal{D}_2)\]

<p>Almost there. We have the two last terms of our target equation already. Let’s move them to the right and see what’s left</p>

\[\log p(\theta \mid \mathcal{D}) = \bigg( \log p(\mathcal{D}_1 \mid \theta) + \log p(\theta) - \log p(\mathcal{D}_1) \bigg) + \log p(\mathcal{D}_2 \mid \theta) - \log p( \mathcal{D}_2)\]

<p>As we can see now, the terms in the big parentheses are the Baye’s Rule expression of the missing term, so we’re done!</p>

\[\log p(\theta \mid \mathcal{D} ) = \log p(\theta \mid \mathcal{D}_1) + \log p(\mathcal{D}_2 \mid \theta) - \log p(\mathcal{D}_2)\]

<h4 id="small-comment">Small Comment</h4>
<p>While prior based methods like EWC are theoretically sound, in practice they are quite bad. I strongly believe this is because the assumptions we make so that computing \(p(\theta | \mathcal{D}_1)\) is tractable are wayy to restrictive. Maybe more flexible posterior inference is a good research direction. More on that in the next blog post!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[EWC posterior derivation for prior-focused methods]]></summary></entry></feed>